{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "loader = TextLoader(\"../state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "hf_embeddings_vector_db = FAISS.from_documents(docs, hf_embeddings)\n",
    "hf_embeddings_vector_db.save_local(\"faiss_index\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'We’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \\n\\nAnd tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.  \\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means demanding more competition. \\n\\nI’m a capitalist, but capitalism without competition isn’t capitalism. \\n\\nIt’s exploitation—and it drives up prices. \\n\\nWhen corporations don’t have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. \\n\\nWe see it happening with ocean carriers moving goods in and out of America. \\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits.'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What did the president say about economics?\"\n",
    "docs = hf_embeddings_vector_db.similarity_search(question)\n",
    "docs[0].page_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "gpt35_azure_llm = AzureChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=\"02e3dbabaf334ccb959cbeadbd3f99c3\",\n",
    "    openai_api_base=\"https://llm-x-gpt.openai.azure.com/\",\n",
    "    deployment_name='LLM-X-GPT35-TURBO',\n",
    "    openai_api_version=\"2023-03-15-preview\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watson/.virtualenvs/QnAEvaluation/lib/python3.9/site-packages/langchain/embeddings/openai.py:214: UserWarning: WARNING! deployment_name is not default parameter.\n",
      "                    deployment_name was transferred to model_kwargs.\n",
      "                    Please confirm that deployment_name is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "openai_azure_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=\"02e3dbabaf334ccb959cbeadbd3f99c3\",\n",
    "    openai_api_base=\"https://llm-x-gpt.openai.azure.com/\",\n",
    "    deployment_name='LLM-X-Embedding'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: [\"1. Can you provide any information on the president's statements regarding the field of economics?\", \"2. I'm interested in knowing the president's views and comments on the subject of economics. Could you share any relevant information?\", '3. Could you please share any insights or remarks made by the president in relation to economics?']\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=hf_embeddings_vector_db.as_retriever(),\n",
    "    llm=gpt35_azure_llm\n",
    ")\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'What did the president say about Zelensky?',\n 'result': 'The president mentioned President Zelenskyy of Ukraine and praised the fearlessness, courage, and determination of the Ukrainian people.'}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gpt35_azure_llm,\n",
    "    retriever=hf_embeddings_vector_db.as_retriever()\n",
    ")\n",
    "\n",
    "question = \"What did the president say about Zelensky?\"\n",
    "qa_rag_chain({\"query\": question})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from langchain.chains import QAGenerationChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "templ = \"\"\"You are a smart assistant designed to help high school teachers come up with reading comprehension questions.\n",
    "Given a piece of text, you must come up with a {k} different question and answer pairs that can be used to test a student's reading comprehension abilities.\n",
    "When coming up with this question/answer pair, each pair must be respond in the following format:\n",
    "\n",
    "{{\n",
    "    \"question\": \"$YOUR_QUESTION_HERE\",\n",
    "    \"answer\": \"$THE_ANSWER_HERE\"\n",
    "}}\n",
    "\n",
    "So in your final answer you should response with a list of {k} pairs in this format:\n",
    "\n",
    "```\n",
    "[{{\n",
    "    \"question\": \"$YOUR_QUESTION_HERE\",\n",
    "    \"answer\": \"$THE_ANSWER_HERE\"\n",
    "}},\n",
    " {{\n",
    "    \"question\": \"$YOUR_QUESTION_HERE\",\n",
    "    \"answer\": \"$THE_ANSWER_HERE\"\n",
    "}},\n",
    " {{\n",
    "    \"question\": \"$YOUR_QUESTION_HERE\",\n",
    "    \"answer\": \"$THE_ANSWER_HERE\"\n",
    "    }}\n",
    "]\n",
    "```\n",
    "\n",
    "Please come up with a list of {k} question/answer pairs, in the specified list of JSONS format, for the following text:\n",
    "----------------\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "multi_qa_prompt = PromptTemplate.from_template(template=templ, partial_variables={\"k\": 5})\n",
    "qa_generation_chain = QAGenerationChain.from_llm(llm=gpt35_azure_llm, prompt=multi_qa_prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'question': 'What is the purpose of the chief prosecutor for pandemic fraud?',\n  'answer': 'To go after the criminals who stole relief money meant for small businesses and Americans.'},\n {'question': 'How much will the deficit be reduced to by the end of this year?',\n  'answer': 'Less than half of what it was before the current president took office.'},\n {'question': \"What does the speaker mean by 'capitalism without competition isn't capitalism'?\",\n  'answer': 'The speaker believes that true capitalism requires competition, and without it, it becomes exploitation.'},\n {'question': \"What happens to prices when corporations don't have to compete?\",\n  'answer': 'Prices go up.'},\n {'question': 'What did foreign-owned ocean carriers do during the pandemic?',\n  'answer': 'They raised prices by as much as 1,000% and made record profits.'}]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_GT = qa_generation_chain.run(docs[0].page_content)[0]\n",
    "qna_GT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA number 1 \n",
      "\n",
      "Question: What is the purpose of the chief prosecutor for pandemic fraud?\n",
      "\n",
      "Answer: To go after the criminals who stole relief money meant for small businesses and Americans.\n",
      "\n",
      "LLM Answer: The purpose of the chief prosecutor for pandemic fraud is to go after the criminals who stole billions in relief money meant for small businesses and millions of Americans. They will be responsible for investigating and prosecuting cases of fraud related to the misuse of funds intended for pandemic relief.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "QA number 2 \n",
      "\n",
      "Question: How much will the deficit be reduced to by the end of this year?\n",
      "\n",
      "Answer: Less than half of what it was before the current president took office.\n",
      "\n",
      "LLM Answer: The given context does not provide specific information about the projected reduction of the deficit by the end of this year. Therefore, I don't have the information to answer your question.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "QA number 3 \n",
      "\n",
      "Question: What does the speaker mean by 'capitalism without competition isn't capitalism'?\n",
      "\n",
      "Answer: The speaker believes that true capitalism requires competition, and without it, it becomes exploitation.\n",
      "\n",
      "LLM Answer: The speaker means that true capitalism requires competition. Without competition, capitalism becomes exploitative and leads to higher prices for consumers. The speaker believes that when corporations don't have to compete, they can drive up prices, harm small businesses, and exploit consumers.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "QA number 4 \n",
      "\n",
      "Question: What happens to prices when corporations don't have to compete?\n",
      "\n",
      "Answer: Prices go up.\n",
      "\n",
      "LLM Answer: When corporations don't have to compete, their profits go up, and as a result, prices go up as well. This lack of competition allows corporations to have more control over the market, leading to higher prices for consumers. Additionally, small businesses and family farmers and ranchers may struggle or go under when they cannot compete with larger, non-competitive corporations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "QA number 5 \n",
      "\n",
      "Question: What did foreign-owned ocean carriers do during the pandemic?\n",
      "\n",
      "Answer: They raised prices by as much as 1,000% and made record profits.\n",
      "\n",
      "LLM Answer: During the pandemic, foreign-owned ocean carriers raised prices by as much as 1,000% and made record profits.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions_and_answers_llm = []\n",
    "for i, qa in enumerate(qna_GT):\n",
    "    question, answer = qa[\"question\"], qa[\"answer\"]\n",
    "    llm_answer = qa_rag_chain({\"query\": question})[\"result\"]\n",
    "    questions_and_answers_llm.append({\"question\": question, \"result\": llm_answer})\n",
    "    print(f\"QA number {i + 1} \\n\")\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(f\"LLM Answer: {llm_answer}\\n\")\n",
    "    print(\"--------------------------------------------------\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "template = \"\"\"You are a teacher grading a quiz.\n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: Correct or Incorrect here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. If the student answers that there is no specific information provided in the context, then the answer is Incorrect. Begin!\n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "Your response should be as follows:\n",
    "\n",
    "GRADE: (Correct or Incorrect)\n",
    "(line break)\n",
    "JUSTIFICATION: (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.)\n",
    "\"\"\"\n",
    "\n",
    "GRADE_ANSWER_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from langchain.evaluation import QAEvalChain\n",
    "\n",
    "eval_prompt = GRADE_ANSWER_PROMPT\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(\n",
    "    llm=gpt35_azure_llm,\n",
    "    prompt=eval_prompt\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'results': \"GRADE: Correct\\n\\nJUSTIFICATION: The student answer accurately states that the purpose of the chief prosecutor for pandemic fraud is to go after the criminals who stole relief money meant for small businesses and Americans. The student's answer also includes additional information about investigating and prosecuting cases of fraud related to the misuse of funds intended for pandemic relief, which does not conflict with the true answer.\"},\n {'results': 'GRADE: Incorrect\\n\\nJUSTIFICATION: The student answer states that there is no specific information provided in the context about the projected reduction of the deficit by the end of this year. However, the true answer does provide specific information, stating that the deficit will be reduced to less than half of what it was before the current president took office.'},\n {'results': 'GRADE: Correct\\n\\nJUSTIFICATION: The student answer accurately explains that the speaker believes capitalism without competition leads to exploitation and higher prices for consumers, which aligns with the true answer.'},\n {'results': \"GRADE: Correct\\n\\nJUSTIFICATION: The student answer accurately states that when corporations don't have to compete, prices go up. Additionally, the student provides additional information about the effects of lack of competition on small businesses and family farmers and ranchers, which does not conflict with the true answer.\"},\n {'results': 'GRADE: Correct\\n\\nJUSTIFICATION: The student answer accurately states that foreign-owned ocean carriers raised prices by as much as 1,000% and made record profits during the pandemic, which matches the true answer.'}]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_outputs = eval_chain.evaluate(\n",
    "    examples=qna_GT,\n",
    "    predictions=questions_and_answers_llm,\n",
    "    question_key=\"question\",\n",
    "    prediction_key=\"result\"\n",
    ")\n",
    "graded_outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADE: Correct\n",
      "\n",
      "JUSTIFICATION: The student answer accurately states that the purpose of the chief prosecutor for pandemic fraud is to go after the criminals who stole relief money meant for small businesses and Americans. The student's answer also includes additional information about investigating and prosecuting cases of fraud related to the misuse of funds intended for pandemic relief, which does not conflict with the true answer.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "GRADE: Incorrect\n",
      "\n",
      "JUSTIFICATION: The student answer states that there is no specific information provided in the context about the projected reduction of the deficit by the end of this year. However, the true answer does provide specific information, stating that the deficit will be reduced to less than half of what it was before the current president took office.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "GRADE: Correct\n",
      "\n",
      "JUSTIFICATION: The student answer accurately explains that the speaker believes capitalism without competition leads to exploitation and higher prices for consumers, which aligns with the true answer.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "GRADE: Correct\n",
      "\n",
      "JUSTIFICATION: The student answer accurately states that when corporations don't have to compete, prices go up. Additionally, the student provides additional information about the effects of lack of competition on small businesses and family farmers and ranchers, which does not conflict with the true answer.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "GRADE: Correct\n",
      "\n",
      "JUSTIFICATION: The student answer accurately states that foreign-owned ocean carriers raised prices by as much as 1,000% and made record profits during the pandemic, which matches the true answer.\n",
      "--------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for qa_grade in graded_outputs:\n",
    "    print(f\"{qa_grade['results']}\")\n",
    "    print(\"--------------------------------------------------\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 0.12518518518518518}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "from langchain.evaluation import EmbeddingDistance\n",
    "\n",
    "evaluator = load_evaluator(evaluator=EvaluatorType.PAIRWISE_STRING_DISTANCE,\n",
    "                           distance_metric=EmbeddingDistance.EUCLIDEAN,\n",
    "                           embeddings=openai_azure_embeddings,\n",
    "                           llm=gpt35_azure_llm)\n",
    "\n",
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Seattle is very hot in June\", prediction_b=\"Seattle is cool in June.\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "{'score': 0.548644889956817}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = load_evaluator(evaluator=EvaluatorType.EMBEDDING_DISTANCE,\n",
    "                           embeddings=HuggingFaceEmbeddings(),\n",
    "                           distance_metric=EmbeddingDistance.COSINE,\n",
    "                           llm=gpt35_azure_llm)\n",
    "\n",
    "evaluator.evaluate_strings(prediction=\"I shall go\", reference=\"I shan't go\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a teacher grading a quiz.\n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: Correct or Incorrect here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. If the student answers that there is no specific information provided in the context, then the answer is Incorrect. Begin!\n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "Your response should include grade and justification in the following JSON format:\n",
    "\n",
    "{{\n",
    "GRADE: (Correct or Incorrect),\n",
    "JUSTIFICATION: (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.)\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "GRADE_ANSWER_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a grader trying to determine if a set of retrieved documents will help a student answer a question. \\n\n",
    "\n",
    "Here is the question: \\n\n",
    "{query}\n",
    "\n",
    "Here are the documents retrieved to answer question: \\n\n",
    "{result}\n",
    "\n",
    "Here is the correct answer to the question: \\n\n",
    "{answer}\n",
    "\n",
    "Criteria:\n",
    "  relevance: Do all of the documents contain information that will help the student arrive that the correct answer to the question?\"\n",
    "\n",
    "Your response should include grade and justification in the following JSON format:\n",
    "\n",
    "{{\n",
    "GRADE: (Correct or Incorrect, depending if all of the documents retrieved meet the criterion),\n",
    "JUSTIFICATION: (Write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Use three sentences maximum. Keep the answer as concise as possible.)\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "GRADE_DOCS_PROMPT = PromptTemplate(input_variables=['result', 'answer', 'query'], template=template)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_grades_from_evaluator_response(retrieved_docs: List[str]) -> list[str]:\n",
    "    parsed_grades = []\n",
    "    for grade_and_justification_json in answers_grade:\n",
    "        grade_and_justification = json.loads(grade_and_justification_json['results'])\n",
    "        grade = grade_and_justification[\"GRADE\"]\n",
    "        parsed_grades.append(grade)\n",
    "    return parsed_grades"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def parse_docs_from_retriver_response(docs: List[str]) -> list[str]:\n",
    "    parsed_docs = []\n",
    "    for doc in docs:\n",
    "        parsed_docs.append(doc.page_content)\n",
    "    return parsed_docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "from prompts import GRADE_DOCS_PROMPT\n",
    "\n",
    "\n",
    "def run_evaluation(qa_rag_chain, retriver, qna_GT):\n",
    "    predictions_list = []\n",
    "    retrieved_docs = []\n",
    "    latencies_list = []\n",
    "\n",
    "    for qna in qna_GT:\n",
    "        question, answer = qna[\"question\"], qna[\"answer\"]\n",
    "        qa_rag_chain_answer = qa_rag_chain({\"query\": question})[\"result\"]\n",
    "        predictions_list.append({\"question\": question, \"answer\": answer, \"result\": qa_rag_chain_answer})\n",
    "        retrieved_docs = retriver.similarity_search(query=question)\n",
    "\n",
    "    parsed_docs = parse_docs_from_retriver_response(retrieved_docs)\n",
    "\n",
    "    retrieval_grade = grade_model_retrieval(qna_GT, parsed_docs)\n",
    "    answers_grade = grade_model_answer(qna_GT, predictions_list)\n",
    "\n",
    "\n",
    "    return answers_grade, retrieval_grade, latencies_list, predictions_list\n",
    "\n",
    "\n",
    "def grade_model_answer(predicted_dataset: List, predictions: List) -> List:\n",
    "\n",
    "    eval_chain = QAEvalChain.from_llm(\n",
    "        llm=gpt35_azure_llm,\n",
    "        prompt=GRADE_ANSWER_PROMPT\n",
    "    )\n",
    "\n",
    "    graded_outputs = eval_chain.evaluate(\n",
    "        predicted_dataset,\n",
    "        predictions,\n",
    "        question_key=\"question\",\n",
    "        prediction_key=\"result\"\n",
    "    )\n",
    "\n",
    "    return graded_outputs\n",
    "\n",
    "\n",
    "def grade_model_retrieval(gt_dataset: List, predictions: List):\n",
    "\n",
    "    eval_chain = QAEvalChain.from_llm(\n",
    "        llm=gpt35_azure_llm,\n",
    "        prompt=GRADE_DOCS_PROMPT\n",
    "    )\n",
    "\n",
    "    graded_outputs = eval_chain.evaluate(\n",
    "        gt_dataset,\n",
    "        predictions,\n",
    "        question_key=\"question\",\n",
    "        prediction_key=\"result\"\n",
    "    )\n",
    "    return graded_outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[77], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m answers_grade, retrieval_grade, latencies_list, predictions_list \u001B[38;5;241m=\u001B[39m \u001B[43mrun_evaluation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqa_rag_chain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                                                                                  \u001B[49m\u001B[43mhf_embeddings_vector_db\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                                                                                  \u001B[49m\u001B[43mqna_GT\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[76], line 17\u001B[0m, in \u001B[0;36mrun_evaluation\u001B[0;34m(qa_rag_chain, retriver, qna_GT)\u001B[0m\n\u001B[1;32m     13\u001B[0m     retrieved_docs \u001B[38;5;241m=\u001B[39m retriver\u001B[38;5;241m.\u001B[39msimilarity_search(query\u001B[38;5;241m=\u001B[39mquestion)\n\u001B[1;32m     15\u001B[0m parsed_docs \u001B[38;5;241m=\u001B[39m parse_docs_from_retriver_response(retrieved_docs)\n\u001B[0;32m---> 17\u001B[0m retrieval_grade \u001B[38;5;241m=\u001B[39m \u001B[43mgrade_model_retrieval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqna_GT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparsed_docs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m answers_grade \u001B[38;5;241m=\u001B[39m grade_model_answer(qna_GT, predictions_list)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m answers_grade, retrieval_grade, latencies_list, predictions_list\n",
      "Cell \u001B[0;32mIn[76], line 48\u001B[0m, in \u001B[0;36mgrade_model_retrieval\u001B[0;34m(gt_dataset, predictions)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgrade_model_retrieval\u001B[39m(gt_dataset: List, predictions: List):\n\u001B[1;32m     43\u001B[0m     eval_chain \u001B[38;5;241m=\u001B[39m QAEvalChain\u001B[38;5;241m.\u001B[39mfrom_llm(\n\u001B[1;32m     44\u001B[0m         llm\u001B[38;5;241m=\u001B[39mgpt35_azure_llm,\n\u001B[1;32m     45\u001B[0m         prompt\u001B[38;5;241m=\u001B[39mGRADE_DOCS_PROMPT\n\u001B[1;32m     46\u001B[0m     )\n\u001B[0;32m---> 48\u001B[0m     graded_outputs \u001B[38;5;241m=\u001B[39m \u001B[43meval_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgt_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquestion_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprediction_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresult\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m graded_outputs\n",
      "File \u001B[0;32m~/.virtualenvs/QnAEvaluation/lib/python3.9/site-packages/langchain/evaluation/qa/eval_chain.py:116\u001B[0m, in \u001B[0;36mQAEvalChain.evaluate\u001B[0;34m(self, examples, predictions, question_key, answer_key, prediction_key, callbacks)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    107\u001B[0m     examples: Sequence[\u001B[38;5;28mdict\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    113\u001B[0m     callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    114\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mdict\u001B[39m]:\n\u001B[1;32m    115\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Evaluate question answering examples and predictions.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 116\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    117\u001B[0m         {\n\u001B[1;32m    118\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m: example[question_key],\n\u001B[1;32m    119\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manswer\u001B[39m\u001B[38;5;124m\"\u001B[39m: example[answer_key],\n\u001B[1;32m    120\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresult\u001B[39m\u001B[38;5;124m\"\u001B[39m: predictions[i][prediction_key],\n\u001B[1;32m    121\u001B[0m         }\n\u001B[1;32m    122\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(examples)\n\u001B[1;32m    123\u001B[0m     ]\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply(inputs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n",
      "File \u001B[0;32m~/.virtualenvs/QnAEvaluation/lib/python3.9/site-packages/langchain/evaluation/qa/eval_chain.py:120\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\n\u001B[1;32m    106\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    107\u001B[0m     examples: Sequence[\u001B[38;5;28mdict\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    113\u001B[0m     callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    114\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mdict\u001B[39m]:\n\u001B[1;32m    115\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Evaluate question answering examples and predictions.\"\"\"\u001B[39;00m\n\u001B[1;32m    116\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    117\u001B[0m         {\n\u001B[1;32m    118\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m: example[question_key],\n\u001B[1;32m    119\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manswer\u001B[39m\u001B[38;5;124m\"\u001B[39m: example[answer_key],\n\u001B[0;32m--> 120\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresult\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mpredictions\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mprediction_key\u001B[49m\u001B[43m]\u001B[49m,\n\u001B[1;32m    121\u001B[0m         }\n\u001B[1;32m    122\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(examples)\n\u001B[1;32m    123\u001B[0m     ]\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply(inputs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n",
      "\u001B[0;31mTypeError\u001B[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "answers_grade, retrieval_grade, latencies_list, predictions_list = run_evaluation(qa_rag_chain,\n",
    "                                                                                  hf_embeddings_vector_db,\n",
    "                                                                                  qna_GT)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
