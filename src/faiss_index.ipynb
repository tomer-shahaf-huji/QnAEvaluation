{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from json import JSONDecodeError\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import QAGenerationChain, RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.evaluation import EmbeddingDistance\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "from langchain.evaluation.schema import StringEvaluator\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "from src.prompt_templates import MULTI_QA_GPT4_PROMPT_TEMPLATE, MULTI_QA_GPT35_PROMPT_TEMPLATE, GRADE_DOCS_PROMPT_TEMPLATE\n",
    "\n",
    "INDEX_OF_FIRST_QNA_IN_RESPONSE = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"../linux-kernel\"\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".c\") and \"/.venv/\" not in dirpath:\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                docs.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                pass\n",
    "print(f\"{len(docs)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4290\n"
     ]
    }
   ],
   "source": [
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.CPP)\n",
    "\n",
    "chunks_cpp_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.CPP, chunk_size=250, chunk_overlap=50\n",
    ")\n",
    "chunks = chunks_cpp_splitter.split_documents(docs)\n",
    "print(f\"{len(chunks)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258\n"
     ]
    }
   ],
   "source": [
    "documents_cpp_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.CPP, chunk_size=4000, chunk_overlap=150\n",
    ")\n",
    "splitted_docs = documents_cpp_splitter.split_documents(docs)\n",
    "print(f\"{len(splitted_docs)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "openai_azure_embeddings = OpenAIEmbeddings(\n",
    "    openai_api_type=\"azure\",\n",
    "    openai_api_key=\"02e3dbabaf334ccb959cbeadbd3f99c3\",\n",
    "    openai_api_base=\"https://llm-x-gpt.openai.azure.com/\",\n",
    "    chunk_size=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "hf_embeddings_vector_db = FAISS.from_documents(chunks, hf_embeddings)\n",
    "hf_embeddings_vector_db.save_local(\"linux-kernel_embeddings\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "question = \"What is the purpose of the get_user_page in the linux kernel?\"\n",
    "retrieved_chunks = hf_embeddings_vector_db.similarity_search(question)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "gpt35_azure_llm = AzureChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=\"02e3dbabaf334ccb959cbeadbd3f99c3\",\n",
    "    openai_api_base=\"https://llm-x-gpt.openai.azure.com/\",\n",
    "    deployment_name='LLM-X-GPT35-TURBO',\n",
    "    openai_api_version=\"2023-03-15-preview\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "gpt4_azure_llm = AzureChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=\"a8d69f68a36b40789df2cc3fdbaacda9\",\n",
    "    openai_api_base=\"https://llmx-gpt-canada-east.openai.azure.com/\",\n",
    "    deployment_name='LLM-X-GPT-4',\n",
    "    openai_api_version=\"2023-03-15-preview\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'What is the purpose of the get_user_page in the linux kernel?',\n 'result': \"The purpose of the `get_user_pages` function in the Linux kernel is to walk a process's page tables and obtain a reference to each `struct page` that corresponds to a user address at a given moment. This function is typically used by the kernel's futex code and is used to access user pages directly. It ensures that the pages are accessible and can be used for various operations, such as IO operations or handling faults.\"}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=gpt35_azure_llm,\n",
    "    retriever=hf_embeddings_vector_db.as_retriever()\n",
    ")\n",
    "\n",
    "qa_rag_chain({\"query\": question})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the function of the get_user_page in the linux kernel?', '2. How does the get_user_page function serve its purpose in the linux kernel?', '3. Can you explain the role and significance of the get_user_page in the linux kernel?']\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=hf_embeddings_vector_db.as_retriever(),\n",
    "    llm=gpt35_azure_llm\n",
    ")\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def create_qna_GT_df(docs: List[Document], num_of_qna_for_doc: int) -> pd.DataFrame:\n",
    "    documents_cpp_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.CPP, chunk_size=4000, chunk_overlap=150\n",
    "    )\n",
    "    splitted_docs = documents_cpp_splitter.split_documents(docs)\n",
    "\n",
    "    multi_qa_prompt = PromptTemplate.from_template(template=MULTI_QA_GPT4_PROMPT_TEMPLATE,\n",
    "                                                   partial_variables={\"k\": num_of_qna_for_doc})\n",
    "    qa_generation_chain = QAGenerationChain.from_llm(llm=gpt4_azure_llm,\n",
    "                                                     prompt=multi_qa_prompt,\n",
    "                                                     text_splitter=documents_cpp_splitter)\n",
    "\n",
    "\n",
    "    qna_GT = []\n",
    "    for splitted_doc in tqdm(splitted_docs):\n",
    "        try:\n",
    "            qna = qa_generation_chain.run(splitted_doc.page_content)[INDEX_OF_FIRST_QNA_IN_RESPONSE]\n",
    "            qna_GT += qna\n",
    "        except JSONDecodeError:\n",
    "            print(\"Failed to generate valid QnA JSON for doc\")\n",
    "\n",
    "    qna_GT_df = pd.DataFrame(qna_GT)\n",
    "    return qna_GT_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def get_qna_with_chain_answers_df(qa_rag_chain: RetrievalQA, qna_GT_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    qna_with_chain_answers = qna_GT_df.copy()\n",
    "    qna_with_chain_answers[\"chain_answer\"] = qna_with_chain_answers.apply(\n",
    "        lambda qna: qa_rag_chain({\"query\": qna[\"question\"]})[\"result\"], axis=1)\n",
    "\n",
    "    return qna_with_chain_answers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def get_evaluator_score(evaluator: StringEvaluator, qna_with_chain_answer: pd.Series) -> float:\n",
    "    grade = evaluator.evaluate_strings(\n",
    "        prediction=qna_with_chain_answer[\"chain_answer\"],\n",
    "        reference=qna_with_chain_answer[\"answer\"],\n",
    "        input=qna_with_chain_answer[\"question\"])\n",
    "\n",
    "    return grade[\"score\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def get_retrieval_score(retriever, qna_with_chain_answer: pd.Series):\n",
    "    GRADE_DOCS_PROMPT = PromptTemplate(input_variables=['result', 'answer', 'query'],\n",
    "                                       template=GRADE_DOCS_PROMPT_TEMPLATE)\n",
    "    retrieval_eval_chain = load_evaluator(\n",
    "        evaluator=EvaluatorType.QA,\n",
    "        llm=gpt35_azure_llm,\n",
    "        prompt=GRADE_DOCS_PROMPT\n",
    "    )\n",
    "\n",
    "    retrieved_docs = retriever.get_relevant_documents(query=qna_with_chain_answer[\"question\"],\n",
    "                                                      search_type=\"similarity_score_threshold\",\n",
    "                                                      search_kwargs={\"k\": 2})\n",
    "\n",
    "    grade = retrieval_eval_chain.evaluate_strings(\n",
    "        prediction=retrieved_docs,\n",
    "        reference=qna_with_chain_answer[\"answer\"],\n",
    "        input=qna_with_chain_answer[\"question\"])\n",
    "\n",
    "    return grade[\"score\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def _fix_embedding_distance_evaluator_score(score: float) -> float:\n",
    "    return round(1 - score, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def get_grades_for_chain_qna(qna_with_chain_answers_df: pd.DataFrame,\n",
    "                             retriever: BaseRetriever = None) -> pd.DataFrame:\n",
    "    grades_for_chain_qna = qna_with_chain_answers_df.copy()\n",
    "\n",
    "    labeled_criteria_evaluator = load_evaluator(evaluator=EvaluatorType.LABELED_CRITERIA,\n",
    "                                                criteria=\"correctness\",\n",
    "                                                llm=gpt35_azure_llm)\n",
    "\n",
    "    embedding_distance_evaluator = load_evaluator(evaluator=EvaluatorType.EMBEDDING_DISTANCE,\n",
    "                                                  distance_metric=EmbeddingDistance.COSINE,\n",
    "                                                  embeddings=hf_embeddings,\n",
    "                                                  llm=gpt35_azure_llm)\n",
    "\n",
    "    qa_llm_jugde_evaluator = load_evaluator(evaluator=EvaluatorType.QA,\n",
    "                                            llm=gpt35_azure_llm)\n",
    "\n",
    "    grades_for_chain_qna[\"labeled_criteria_grades\"] = qna_with_chain_answers_df.apply(\n",
    "        lambda qna_with_chain_answer: get_evaluator_score(\n",
    "            evaluator=labeled_criteria_evaluator,\n",
    "            qna_with_chain_answer=qna_with_chain_answer), axis=1)\n",
    "\n",
    "    grades_for_chain_qna[\"qa_llm_jugde_grades\"] = qna_with_chain_answers_df.apply(\n",
    "        lambda qna_with_chain_answer: get_evaluator_score(\n",
    "            evaluator=qa_llm_jugde_evaluator,\n",
    "            qna_with_chain_answer=qna_with_chain_answer), axis=1)\n",
    "\n",
    "    grades_for_chain_qna[\"embedding_distance_grades\"] = qna_with_chain_answers_df.apply(\n",
    "        lambda qna_with_chain_answer: _fix_embedding_distance_evaluator_score(\n",
    "            get_evaluator_score(evaluator=embedding_distance_evaluator,\n",
    "                                qna_with_chain_answer=qna_with_chain_answer)), axis=1)\n",
    "\n",
    "    if retriever is not None:\n",
    "        grades_for_chain_qna[\"retrieval_score\"] = qna_with_chain_answers_df.apply(\n",
    "            lambda qna_with_chain_answer: get_retrieval_score(\n",
    "                retriever=retriever,\n",
    "                qna_with_chain_answer=qna_with_chain_answer), axis=1)\n",
    "\n",
    "    return grades_for_chain_qna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def run_evaluation(qa_rag_chain: RetrievalQA, qna_GT_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    qna_with_chain_answers_df = get_qna_with_chain_answers_df(qa_rag_chain, qna_GT_df)\n",
    "    grades_for_chain_qna = \\\n",
    "        get_grades_for_chain_qna(qna_with_chain_answers_df=qna_with_chain_answers_df,\n",
    "                                 retriever=qa_rag_chain.retriever)\n",
    "    return grades_for_chain_qna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watson/.virtualenvs/QnAEvaluation/lib/python3.9/site-packages/langchain/evaluation/schema.py:104: UserWarning: Ignoring input in EmbeddingDistanceEvalChain, as it is not expected.\n",
      "  warn(self._skip_input_warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                            question  \\\n0  What is the purpose of the 'for' loop in the g...   \n1  What does the variable 'fact' represent in the...   \n2             What is the purpose of the given code?   \n3    What is the return type of the 'fact' function?   \n4           What is the purpose of the code snippet?   \n5  What is the value of 'fact' after the code sni...   \n\n                                              answer  \\\n0  The 'for' loop is used to calculate the factor...   \n1  The variable 'fact' stores the factorial of th...   \n2  The purpose of the given code is to calculate ...   \n3   The return type of the 'fact' function is 'int'.   \n4  To calculate and display the factorial of a nu...   \n5    The factorial of the number entered by the user   \n\n                                        chain_answer  labeled_criteria_grades  \\\n0  The purpose of the 'for' loop in the given cod...                      NaN   \n1  In the given code, the variable 'fact' represe...                      1.0   \n2  The purpose of the given code is to input the ...                      NaN   \n3   The return type of the 'fact' function is 'int'.                      1.0   \n4  The purpose of the code snippet is to prompt t...                      NaN   \n5  The value of 'fact' cannot be determined witho...                      1.0   \n\n   embedding_distance_grades  qa_llm_jugde_grades  retrieval_score  \n0               3.103038e-01                    1              NaN  \n1               1.048536e-01                    1              1.0  \n2               6.754299e-01                    0              1.0  \n3              -2.220446e-16                    1              1.0  \n4               7.435595e-01                    0              NaN  \n5               5.335894e-01                    1              1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>chain_answer</th>\n      <th>labeled_criteria_grades</th>\n      <th>embedding_distance_grades</th>\n      <th>qa_llm_jugde_grades</th>\n      <th>retrieval_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the purpose of the 'for' loop in the g...</td>\n      <td>The 'for' loop is used to calculate the factor...</td>\n      <td>The purpose of the 'for' loop in the given cod...</td>\n      <td>NaN</td>\n      <td>3.103038e-01</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What does the variable 'fact' represent in the...</td>\n      <td>The variable 'fact' stores the factorial of th...</td>\n      <td>In the given code, the variable 'fact' represe...</td>\n      <td>1.0</td>\n      <td>1.048536e-01</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is the purpose of the given code?</td>\n      <td>The purpose of the given code is to calculate ...</td>\n      <td>The purpose of the given code is to input the ...</td>\n      <td>NaN</td>\n      <td>6.754299e-01</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is the return type of the 'fact' function?</td>\n      <td>The return type of the 'fact' function is 'int'.</td>\n      <td>The return type of the 'fact' function is 'int'.</td>\n      <td>1.0</td>\n      <td>-2.220446e-16</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the purpose of the code snippet?</td>\n      <td>To calculate and display the factorial of a nu...</td>\n      <td>The purpose of the code snippet is to prompt t...</td>\n      <td>NaN</td>\n      <td>7.435595e-01</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>What is the value of 'fact' after the code sni...</td>\n      <td>The factorial of the number entered by the user</td>\n      <td>The value of 'fact' cannot be determined witho...</td>\n      <td>1.0</td>\n      <td>5.335894e-01</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_evaluation(qa_rag_chain=qa_rag_chain, qna_GT_df=qna_GT_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 24/258 [03:34<41:36, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 51/258 [07:01<23:50,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 91/258 [11:58<24:41,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 118/258 [15:14<17:30,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 135/258 [17:29<18:51,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 170/258 [21:48<13:28,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 190/258 [24:36<08:56,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 220/258 [28:38<05:00,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 235/258 [30:45<03:22,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 236/258 [30:52<03:02,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 238/258 [31:13<03:03,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate valid QnA JSON for doc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258/258 [33:36<00:00,  7.82s/it]\n"
     ]
    }
   ],
   "source": [
    "qna_GT_df = create_qna_GT_df(docs=docs, num_of_qna_for_doc=3)\n",
    "qna_GT_df.to_csv(\"qna_GT_df.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QAEvalChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mQAEvalChain\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'QAEvalChain' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import QAEvalChain\n",
    "\n",
    "template = \"\"\"You are a teacher grading a quiz on the Linux kernel.\n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin!\n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"result\", \"answer\"], template=template\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
